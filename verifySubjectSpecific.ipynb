{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter requires_grad status:\n",
      "spatio_temporal.weight: requires_grad=False\n",
      "spatio_temporal.bias: requires_grad=False\n",
      "batch_norm.weight: requires_grad=False\n",
      "batch_norm.bias: requires_grad=False\n",
      "fc_layers.subject_1.weight: requires_grad=True\n",
      "fc_layers.subject_1.bias: requires_grad=True\n",
      "fc_layers.subject_2.weight: requires_grad=True\n",
      "fc_layers.subject_2.bias: requires_grad=True\n",
      "fc_layers.subject_3.weight: requires_grad=True\n",
      "fc_layers.subject_3.bias: requires_grad=True\n",
      "fc_layers.subject_4.weight: requires_grad=True\n",
      "fc_layers.subject_4.bias: requires_grad=True\n",
      "fc_layers.subject_5.weight: requires_grad=True\n",
      "fc_layers.subject_5.bias: requires_grad=True\n",
      "fc_layers.subject_6.weight: requires_grad=True\n",
      "fc_layers.subject_6.bias: requires_grad=True\n",
      "fc_layers.subject_7.weight: requires_grad=True\n",
      "fc_layers.subject_7.bias: requires_grad=True\n",
      "fc_layers.subject_8.weight: requires_grad=True\n",
      "fc_layers.subject_8.bias: requires_grad=True\n",
      "fc_layers.subject_9.weight: requires_grad=True\n",
      "fc_layers.subject_9.bias: requires_grad=True\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0\n",
      "Gradient norm for subject_3: 912.0909423828125\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 0.0\n",
      "Biases changed for subject_2: 0.0\n",
      "\n",
      "Weights changed for subject_3: 1396.0\n",
      "Biases changed for subject_3: 4.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n",
      "\n",
      "Test passed: Only the weights and biases corresponding to the specific subject have changed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from shallow import SubjectDicionaryFCNet\n",
    "\n",
    "# Initialize the model\n",
    "model = SubjectDicionaryFCNet(n_chans=22, n_outputs=4)\n",
    "\n",
    "# Set all weights and biases of fc_layers to ones for consistency\n",
    "for fc_layer in model.fc_layers.values():\n",
    "    fc_layer.weight.data.fill_(1.0)\n",
    "    fc_layer.bias.data.fill_(1.0)\n",
    "\n",
    "# First, set requires_grad=False for all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Then, set requires_grad=True for parameters in fc_layers\n",
    "for fc_layer in model.fc_layers.values():\n",
    "    for param in fc_layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify requires_grad status\n",
    "print(\"Parameter requires_grad status:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Copy initial weights and biases\n",
    "initial_weights = {}\n",
    "initial_biases = {}\n",
    "for subject_id, fc_layer in model.fc_layers.items():\n",
    "    initial_weights[subject_id] = fc_layer.weight.data.clone()\n",
    "    initial_biases[subject_id] = fc_layer.bias.data.clone()\n",
    "\n",
    "# Prepare input data\n",
    "batch_size = 5\n",
    "n_chans = 22\n",
    "n_times = 1001\n",
    "\n",
    "# Create a batch with zeros\n",
    "batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "\n",
    "\n",
    "# Define target outputs\n",
    "n_outputs = model.fc_layers['subject_1'].out_features\n",
    "target = torch.zeros(batch_size, n_outputs)\n",
    "\n",
    "# Use a non-zero target for one specific subject\n",
    "specific_subject_id = 3  # Subject ID for which we want to induce updates\n",
    "#fill with 1's\n",
    "batch[specific_subject_id - 1] = 1\n",
    "target[specific_subject_id - 1] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Arbitrary non-zero target\n",
    "\n",
    "# Set the last time point to contain the subject IDs (from 1 to batch_size)\n",
    "for i in range(batch_size):\n",
    "    subject_id = i + 1  # Subject IDs from 1 to 5  \n",
    "    batch[i, :, -1] = 3 * 1000000\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (only parameters with requires_grad=True will be updated)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "output = model(batch)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Verify gradients before optimizer step\n",
    "print(\"\\nGradients before optimizer step:\")\n",
    "for subject_id, fc_layer in model.fc_layers.items():\n",
    "    grad_norm = fc_layer.weight.grad.norm().item() if fc_layer.weight.grad is not None else 0\n",
    "    print(f\"Gradient norm for {subject_id}: {grad_norm}\")\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# Check which fc_layers have had their weights updated\n",
    "for subject_id, fc_layer in model.fc_layers.items():\n",
    "    weights_changed = (fc_layer.weight.data != initial_weights[subject_id]).float().sum().item()\n",
    "    biases_changed = (fc_layer.bias.data != initial_biases[subject_id]).float().sum().item()\n",
    "    print(f\"\\nWeights changed for {subject_id}: {weights_changed}\")\n",
    "    print(f\"Biases changed for {subject_id}: {biases_changed}\")\n",
    "\n",
    "# Assertions to verify subject specificity\n",
    "for subject_id, fc_layer in model.fc_layers.items():\n",
    "    weights_changed = (fc_layer.weight.data != initial_weights[subject_id]).float().sum().item()\n",
    "    biases_changed = (fc_layer.bias.data != initial_biases[subject_id]).float().sum().item()\n",
    "    if subject_id == f'subject_{specific_subject_id}':\n",
    "        assert weights_changed > 0, f\"Weights for {subject_id} did not change!\"\n",
    "        assert biases_changed > 0, f\"Biases for {subject_id} did not change!\"\n",
    "    else:\n",
    "        assert weights_changed == 0, f\"Weights for {subject_id} have changed!\"\n",
    "        assert biases_changed == 0, f\"Biases for {subject_id} have changed!\"\n",
    "\n",
    "print(\"\\nTest passed: Only the weights and biases corresponding to the specific subject have changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter requires_grad status:\n",
      "spatio_temporal.weight: requires_grad=False\n",
      "spatio_temporal.bias: requires_grad=False\n",
      "batch_norm.weight: requires_grad=False\n",
      "batch_norm.bias: requires_grad=False\n",
      "fc.weight: requires_grad=True\n",
      "fc.bias: requires_grad=True\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for fc: 654.693115234375\n",
      "\n",
      "Total number of weights: 1440\n",
      "Total number of biases: 4\n",
      "\n",
      "Weights changed: 1408.0\n",
      "Biases changed: 4.0\n",
      "tensor([[0.9957, 1.0033, 0.9962,  ..., 1.0023, 0.9964, 0.9964],\n",
      "        [0.9956, 1.0033, 0.9961,  ..., 1.0023, 0.9963, 0.9963],\n",
      "        [0.9956, 1.0033, 0.9961,  ..., 1.0023, 0.9963, 0.9962],\n",
      "        [0.9955, 1.0033, 0.9960,  ..., 1.0023, 0.9962, 0.9962]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from shallow import CollapsedShallowNet\n",
    "\n",
    "# Initialize the model\n",
    "model = CollapsedShallowNet(n_chans=22, n_outputs=4)\n",
    "\n",
    "fc_layer = model.fc\n",
    "# Set all weights and biases of fc to ones for consistency\n",
    "\n",
    "dummy_input = torch.zeros(1, 22, 1001)  # Adjust shape as per model requirements\n",
    "model(dummy_input)\n",
    "\n",
    "nn.init.constant_(fc_layer.weight, 1.0)\n",
    "nn.init.constant_(fc_layer.bias, 1.0)\n",
    "\n",
    "# First, set requires_grad=False for all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Then, set requires_grad=True for parameters in fc\n",
    "for param in fc_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Verify requires_grad status\n",
    "print(\"Parameter requires_grad status:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Copy initial weights and biases\n",
    "\n",
    "\n",
    "initial_weights = {}\n",
    "initial_biases = {}\n",
    "\n",
    "initial_weights[1] = fc_layer.weight.data.clone()\n",
    "initial_biases[1] = fc_layer.bias.data.clone()\n",
    "\n",
    "\n",
    "# Prepare input data\n",
    "batch_size = 5\n",
    "n_chans = 22\n",
    "n_times = 1001\n",
    "\n",
    "# Create a batch with zeros\n",
    "batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "\n",
    "# Define target outputs\n",
    "n_outputs = fc_layer.out_features\n",
    "target = torch.zeros(batch_size, n_outputs)\n",
    "\n",
    "# Use a non-zero target for one specific subject\n",
    "specific_subject_id = 3  # Subject ID for which we want to induce updates\n",
    "#fill with 1's\n",
    "batch[specific_subject_id - 1] = 1\n",
    "target[specific_subject_id - 1] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Arbitrary non-zero target\n",
    "\n",
    "# Set the last time point to contain the subject IDs (from 1 to batch_size)\n",
    "for i in range(batch_size):\n",
    "    subject_id = i + 1  # Subject IDs from 1 to 5\n",
    "    batch[i, :, -1] = 3 * 1000000\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (only parameters with requires_grad=True will be updated)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "output = model(batch)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Verify gradients before optimizer step\n",
    "print(\"\\nGradients before optimizer step:\")\n",
    "\n",
    "grad_norm = fc_layer.weight.grad.norm().item() if fc_layer.weight.grad is not None else 0\n",
    "print(f\"Gradient norm for fc: {grad_norm}\")\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# check total amount of weights\n",
    "total_weights = fc_layer.weight.data.numel()\n",
    "total_biases = fc_layer.bias.data.numel()\n",
    "print(f\"\\nTotal number of weights: {total_weights}\")\n",
    "print(f\"Total number of biases: {total_biases}\")\n",
    "\n",
    "\n",
    "#check how many weights have changed\n",
    "weights_changed = (fc_layer.weight.data != initial_weights[1]).float().sum().item()\n",
    "biases_changed = (fc_layer.bias.data != initial_biases[1]).float().sum().item()\n",
    "print(f\"\\nWeights changed: {weights_changed}\")\n",
    "print(f\"Biases changed: {biases_changed}\")\n",
    "\n",
    "\n",
    "\n",
    "# Assertions to verify that all weights and biases have changed\n",
    "assert weights_changed > total_weights-100, \"Not all weights have changed!\"\n",
    "assert biases_changed == total_biases, \"Not all biases have changed!\"\n",
    "\n",
    "print(fc_layer.weight.data[:8])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Weights changed: 2048.0, Biases changed: 8.0\n",
      "Iteration 2: Weights changed: 2172.0, Biases changed: 8.0\n",
      "Iteration 3: Weights changed: 2184.0, Biases changed: 8.0\n",
      "Iteration 4: Weights changed: 2184.0, Biases changed: 8.0\n",
      "Iteration 5: Weights changed: 2184.0, Biases changed: 8.0\n",
      "Iteration 6: Weights changed: 2184.0, Biases changed: 8.0\n",
      "Iteration 7: Weights changed: 2184.0, Biases changed: 8.0\n",
      "Iteration 8: Weights changed: 2184.0, Biases changed: 8.0\n",
      "Iteration 9: Weights changed: 2184.0, Biases changed: 8.0\n",
      "Iteration 10: Weights changed: 2184.0, Biases changed: 8.0\n",
      "\n",
      "Final weights and biases after multiple iterations:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1.0836, 0.4521, 0.4649, 0.4327, 0.8600, 0.8812, 0.4191, 0.6643])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([0.5712, 0.5712, 1.0000, 0.5712, 1.0000, 1.0000, 0.5712, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from shallow import SubjectOneHotNet\n",
    "\n",
    "# Initialize the model\n",
    "model = SubjectOneHotNet(n_chans=22, n_outputs=4)\n",
    "\n",
    "# Set all weights and biases of fc_layers to ones for consistency\n",
    "model.fc_shared.weight.data.fill_(1.0)\n",
    "model.fc_shared.bias.data.fill_(1.0)\n",
    "\n",
    "# Set requires_grad=False for all parameters except the fc_shared layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.fc_shared.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Prepare input data\n",
    "batch_size = 5\n",
    "n_chans = 22\n",
    "n_times = 1001\n",
    "n_outputs = model.n_outputs\n",
    "\n",
    "# Initialize weights and biases tracking for multiple iterations\n",
    "initial_weights = model.fc_shared.weight.data.clone()\n",
    "initial_biases = model.fc_shared.bias.data.clone()\n",
    "weights_change_history = []\n",
    "biases_change_history = []\n",
    "\n",
    "# Create a batch with zeros\n",
    "batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "target = torch.zeros(batch_size, n_outputs)\n",
    "\n",
    "# Choose the specific subject to update\n",
    "specific_subject_id = 2  # Subject ID to test\n",
    "batch[specific_subject_id - 1] = 1\n",
    "\n",
    "# Assign subject IDs to the last time point\n",
    "for i in range(batch_size):\n",
    "    subject_id = i + 1\n",
    "    if i == specific_subject_id - 1:\n",
    "        batch[i, :, -1] = 9 * 1000000\n",
    "    else:\n",
    "        batch[i, :, -1] = 2 * 1000000\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Hook to zero gradients for non-targeted subjects\n",
    "# def gradient_hook(grad):\n",
    "#     mask = torch.zeros_like(grad)\n",
    "#     start_idx = (specific_subject_id - 1) * n_outputs\n",
    "#     end_idx = start_idx + n_outputs\n",
    "#     mask[start_idx:end_idx, :] = 1\n",
    "#     return grad * mask\n",
    "\n",
    "# model.fc_shared.weight.register_hook(gradient_hook)\n",
    "\n",
    "# Run multiple training iterations\n",
    "iterations = 10  # Define the number of training steps\n",
    "for i in range(iterations):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(batch)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    # Record gradients and update\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track changed weights and biases\n",
    "    weights_changed = (model.fc_shared.weight.data != initial_weights).float().sum().item()\n",
    "    biases_changed = (model.fc_shared.bias.data != initial_biases).float().sum().item()\n",
    "    weights_change_history.append(weights_changed)\n",
    "    biases_change_history.append(biases_changed)\n",
    "    batch[specific_subject_id - 1] = 10 ** i\n",
    "    batch[1, :, -1] = 2 * 1000000\n",
    "    #batch[specific_subject_id - 1] = 10 *i\n",
    "\n",
    "    print(f\"Iteration {i + 1}: Weights changed: {weights_changed}, Biases changed: {biases_changed}\")\n",
    "\n",
    "# Display final weight change distribution\n",
    "print(\"\\nFinal weights and biases after multiple iterations:\")\n",
    "for i in range(36):\n",
    "    if i % 4 == 0:\n",
    "        print(model.fc_shared.weight.data[i][:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter requires_grad status:\n",
      "spatio_temporal.weight: requires_grad=False\n",
      "spatio_temporal.bias: requires_grad=False\n",
      "batch_norm.weight: requires_grad=False\n",
      "batch_norm.bias: requires_grad=False\n",
      "fc_shared.weight: requires_grad=True\n",
      "fc_shared.bias: requires_grad=True\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for fc: 205.77197265625\n",
      "\n",
      "Total number of weights: 12960\n",
      "Total number of biases: 36\n",
      "\n",
      "Weights changed: 2072.0\n",
      "Biases changed: 8.0\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([0.9200, 1.0000, 1.0000, 0.9200, 1.0000, 1.0000, 0.9200, 0.9200])\n",
      "tensor([0.9770, 0.9830, 0.9830, 0.9770, 0.9940, 0.9890, 0.9830, 0.9940])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from shallow import SubjectAdvIndexFCNet\n",
    "\n",
    "# Initialize the model\n",
    "model = SubjectAdvIndexFCNet(n_chans=22, n_outputs=4)\n",
    "\n",
    "# Set all weights and biases of fc_layers to ones for consistency\n",
    "\n",
    "model.fc_shared.weight.data.fill_(1.0)\n",
    "model.fc_shared.bias.data.fill_(1.0)\n",
    "\n",
    "# First, set requires_grad=False for all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Then, set requires_grad=True for parameters in fc_layers\n",
    "for param in model.fc_shared.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Verify requires_grad status\n",
    "print(\"Parameter requires_grad status:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Copy initial weights and biases\n",
    "\n",
    "initial_weights = {}\n",
    "initial_biases = {}\n",
    "\n",
    "initial_weights[1] = model.fc_shared.weight.data.clone()\n",
    "initial_biases[1] = model.fc_shared.bias.data.clone()\n",
    "\n",
    "\n",
    "# Prepare input data\n",
    "batch_size = 5\n",
    "n_chans = 22\n",
    "n_times = 1001\n",
    "\n",
    "# Create a batch with zeros\n",
    "batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "\n",
    "# Correct the n_outputs\n",
    "n_outputs = model.n_outputs  # This ensures n_outputs is 4\n",
    "target = torch.zeros(batch_size, n_outputs)\n",
    "\n",
    "# Use a non-zero target for one specific subject\n",
    "specific_subject_id = 5  # Subject ID for which we want to induce updates\n",
    "batch[specific_subject_id - 1] = -90000\n",
    "#batch[specific_subject_id] = 1\n",
    "target[specific_subject_id - 1] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Now this works\n",
    "\n",
    "\n",
    "# Set the last time point to contain the subject IDs (from 1 to batch_size)\n",
    "for i in range(batch_size):\n",
    "    subject_id = i + 1 \n",
    "    if i == specific_subject_id - 1:\n",
    "        batch[i, :, -1] = 4 * 1000000\n",
    "    else: batch[i, :, -1] = specific_subject_id * 1000000\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (only parameters with requires_grad=True will be updated)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "output = model(batch)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Verify gradients before optimizer step\n",
    "print(\"\\nGradients before optimizer step:\")\n",
    "\n",
    "grad_norm = model.fc_shared.weight.grad.norm().item() if model.fc_shared.weight.grad is not None else 0\n",
    "print(f\"Gradient norm for fc: {grad_norm}\")\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# check total amount of weights\n",
    "total_weights = model.fc_shared.weight.data.numel()\n",
    "total_biases = model.fc_shared.bias.data.numel()\n",
    "print(f\"\\nTotal number of weights: {total_weights}\")\n",
    "print(f\"Total number of biases: {total_biases}\")\n",
    "\n",
    "\n",
    "#check how many weights have changed\n",
    "weights_changed = (model.fc_shared.weight.data != initial_weights[1]).float().sum().item()\n",
    "biases_changed = (model.fc_shared.bias.data != initial_biases[1]).float().sum().item()\n",
    "print(f\"\\nWeights changed: {weights_changed}\")\n",
    "print(f\"Biases changed: {biases_changed}\")\n",
    "\n",
    "\n",
    "# Assertions to verify that all weights and biases have changed\n",
    "assert (weights_changed < (total_weights / 9)*3) and  (weights_changed > (total_weights / 9)), \"The interval of weights changed is not correct!\"\n",
    "\n",
    "# exactly8 biases are changed\n",
    "\n",
    "assert biases_changed == 8, \"The number of biases does not correspond!\"\n",
    "\n",
    "\n",
    "\n",
    "# Running with 2 different subjects, I expect arounf 2 / 9 of the weights to change as well as 8 / 36 biases\n",
    "\n",
    "\n",
    "for i in range(36):\n",
    "    if i % 4 == 0:\n",
    "        print(model.fc_shared.weight.data[i][:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter requires_grad status:\n",
      "spatio_temporal.weight: requires_grad=True\n",
      "spatio_temporal.bias: requires_grad=True\n",
      "batch_norm.weight: requires_grad=True\n",
      "batch_norm.bias: requires_grad=True\n",
      "fc.weight: requires_grad=False\n",
      "fc.bias: requires_grad=False\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for spatio_temporal_layer: 0.022053884342312813\n",
      "Gradient norm for batch_norm layer: 0.09037645906209946\n",
      "\n",
      "Total number of weights: 22000\n",
      "Total number of biases: 40\n",
      "\n",
      "Weights changed: 19888.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from shallow import CollapsedShallowNet\n",
    "\n",
    "# Initialize the model\n",
    "model = CollapsedShallowNet(n_chans=22, n_outputs=4)\n",
    "\n",
    "# Get the spatio_temporal layer\n",
    "spatio_temporal_layer = model.spatio_temporal\n",
    "\n",
    "# Set all weights and biases of spatio_temporal_layer to ones for consistency\n",
    "nn.init.constant_(spatio_temporal_layer.weight, 1.0)\n",
    "if spatio_temporal_layer.bias is not None:\n",
    "    nn.init.constant_(spatio_temporal_layer.bias, 1.0)\n",
    "\n",
    "# First, set requires_grad=False for all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Then, set requires_grad=True for parameters in spatio_temporal_layer\n",
    "for param in spatio_temporal_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in model.batch_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify requires_grad status\n",
    "print(\"Parameter requires_grad status:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Copy initial weights and biases\n",
    "initial_weights = {}\n",
    "initial_biases = {}\n",
    "initial_weights[1] = spatio_temporal_layer.weight.data.clone()\n",
    "if spatio_temporal_layer.bias is not None:\n",
    "    initial_biases[1] = spatio_temporal_layer.bias.data.clone()\n",
    "\n",
    "# Prepare input data\n",
    "batch_size = 5\n",
    "n_chans = 22\n",
    "n_times = 1001\n",
    "\n",
    "# Create a batch with zeros\n",
    "batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "\n",
    "# Use a non-zero input for one specific subject to induce updates\n",
    "specific_subject_id = 3  # Subject ID for which we want to induce updates\n",
    "\n",
    "\n",
    "#make each channel of the specific subject 1 be different numbers\n",
    "for i in range(n_chans):\n",
    "    batch[specific_subject_id - 1, i] = i + 1000\n",
    "\n",
    "\n",
    "# Define target outputs\n",
    "n_outputs = model.fc.out_features\n",
    "target = torch.zeros(batch_size, n_outputs)\n",
    "target[specific_subject_id - 1] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Arbitrary non-zero target\n",
    "\n",
    "# Set the last time point to contain the subject IDs (from 1 to batch_size)\n",
    "for i in range(batch_size):\n",
    "    subject_id = i + 1  # Subject IDs from 1 to 5\n",
    "    batch[i, :, -1] = 3 * 1000000\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define an optimizer (only parameters with requires_grad=True will be updated)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "output = model(batch)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Verify gradients before optimizer step\n",
    "print(\"\\nGradients before optimizer step:\")\n",
    "grad_norm_st = spatio_temporal_layer.weight.grad.norm().item() if spatio_temporal_layer.weight.grad is not None else 0\n",
    "grad_norm_bn = model.batch_norm.weight.grad.norm().item() if model.batch_norm.weight.grad is not None else 0\n",
    "print(f\"Gradient norm for spatio_temporal_layer: {grad_norm_st}\")\n",
    "print(f\"Gradient norm for batch_norm layer: {grad_norm_bn}\")\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# Check total amount of weights and biases\n",
    "total_weights = spatio_temporal_layer.weight.data.numel()\n",
    "total_biases = spatio_temporal_layer.bias.data.numel() if spatio_temporal_layer.bias is not None else 0\n",
    "print(f\"\\nTotal number of weights: {total_weights}\")\n",
    "print(f\"Total number of biases: {total_biases}\")\n",
    "\n",
    "# Check how many weights and biases have changed\n",
    "weights_changed = (spatio_temporal_layer.weight.data != initial_weights[1]).float().sum().item()\n",
    "biases_changed = 0\n",
    "if spatio_temporal_layer.bias is not None:\n",
    "    biases_changed = (spatio_temporal_layer.bias.data != initial_biases[1]).float().sum().item()\n",
    "print(f\"\\nWeights changed: {weights_changed}\")\n",
    "\n",
    "# Assertions to verify that all weights and biases have changed\n",
    "assert weights_changed > total_weights-6000, \"Not all weights have changed!\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights changed for subject 1: 0.0\n",
      "Weights changed for subject 2: 0.0\n",
      "Weights changed for subject 3: 17600.0\n",
      "Weights changed for subject 4: 0.0\n",
      "Weights changed for subject 5: 15400.0\n",
      "Weights changed for subject 6: 0.0\n",
      "Weights changed for subject 7: 0.0\n",
      "Weights changed for subject 8: 0.0\n",
      "Weights changed for subject 9: 0.0\n",
      "Total weights per subject: 22000\n",
      "\n",
      "Test passed: Only the weights for the specific subjects have changed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from shallow import SubjectOneHotConvNet\n",
    "\n",
    "# Initialize the model\n",
    "n_chans = 22\n",
    "n_outputs = 4\n",
    "num_subjects = 9\n",
    "model = SubjectOneHotConvNet(n_chans=n_chans, n_outputs=n_outputs, num_subjects=num_subjects)\n",
    "\n",
    "# Initialize the spatio_temporal layer's weights to ones for consistency\n",
    "nn.init.constant_(model.spatio_temporal.weight, 1.0)\n",
    "if model.spatio_temporal.bias is not None:\n",
    "    nn.init.constant_(model.spatio_temporal.bias, 0.0)\n",
    "\n",
    "# Copy initial weights for comparison after training\n",
    "initial_weights = model.spatio_temporal.weight.data.clone()\n",
    "\n",
    "# Set requires_grad=True only for the spatio_temporal layer's parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.spatio_temporal.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Prepare input data for two subjects\n",
    "batch_size = 3  \n",
    "n_times = 1001\n",
    "\n",
    "batch = torch.zeros(batch_size, n_chans, n_times)  # Random non-zero input data\n",
    "for i in range(n_chans):\n",
    "    batch[0, i] = i + 1000  # Subject 3\n",
    "    batch[1, i] = (i + 1000) * 10  # Subject 5\n",
    "\n",
    "# Set the last time point to contain the subject IDs\n",
    "subject_ids = [3, 5, 2]  # Subject IDs present in the batch (between 1 and num_subjects)\n",
    "for i in range(batch_size):\n",
    "    batch[i, :, -1] = subject_ids[i] * 1000000\n",
    "\n",
    "# Define target outputs\n",
    "target = torch.randn(batch_size, n_outputs)  # Random target output\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "output = model(batch)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# Compare weights before and after the update\n",
    "updated_weights = model.spatio_temporal.weight.data\n",
    "weight_diff = updated_weights - initial_weights\n",
    "\n",
    "# Reshape weights to separate them by subject\n",
    "weight_shape = model.spatio_temporal.weight.shape  # (out_channels, in_channels, H, W)\n",
    "out_channels = weight_shape[0]\n",
    "in_channels = weight_shape[1]\n",
    "kernel_height = weight_shape[2]\n",
    "kernel_width = weight_shape[3]\n",
    "\n",
    "# Reshape weights to (num_subjects, num_kernels, in_channels, kernel_height, kernel_width)\n",
    "weights_reshaped = updated_weights.view(num_subjects, model.num_kernels, in_channels, kernel_height, kernel_width)\n",
    "initial_weights_reshaped = initial_weights.view(num_subjects, model.num_kernels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "# Compute the number of weights changed for each subject\n",
    "weights_changed_per_subject = ((weights_reshaped != initial_weights_reshaped).float().sum(dim=(1,2,3,4))).numpy()\n",
    "\n",
    "# Print the number of weights changed per subject\n",
    "for subj_idx in range(num_subjects):\n",
    "    print(f\"Weights changed for subject {subj_idx+1}: {weights_changed_per_subject[subj_idx]}\")\n",
    "\n",
    "# Verify that only the weights for the specific subjects have changed\n",
    "total_weights_per_subject = weights_reshaped[0].numel()\n",
    "print(f\"Total weights per subject: {total_weights_per_subject}\")\n",
    "\n",
    "# Assertions\n",
    "for subj_idx in range(num_subjects):\n",
    "    if subj_idx + 1 in subject_ids and subj_idx + 1 != 2:\n",
    "        assert weights_changed_per_subject[subj_idx] > 0, f\"Weights for subject {subj_idx+1} did not change but should have!\"\n",
    "    else:\n",
    "        assert weights_changed_per_subject[subj_idx] == 0, f\"Weights for subject {subj_idx+1} changed but should not have!\"\n",
    "\n",
    "print(\"\\nTest passed: Only the weights for the specific subjects have changed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter requires_grad status:\n",
      "spatio_temporal_layers.subject_1.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_1.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_2.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_2.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_3.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_3.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_4.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_4.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_5.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_5.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_6.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_6.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_7.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_7.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_8.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_8.bias: requires_grad=True\n",
      "spatio_temporal_layers.subject_9.weight: requires_grad=True\n",
      "spatio_temporal_layers.subject_9.bias: requires_grad=True\n",
      "batch_norm.weight: requires_grad=False\n",
      "batch_norm.bias: requires_grad=False\n",
      "fc.weight: requires_grad=False\n",
      "fc.bias: requires_grad=False\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0.0\n",
      "Gradient norm for subject_2: 0.0\n",
      "Gradient norm for subject_3: 4.1671849970725816e-08\n",
      "Gradient norm for subject_4: 0.0\n",
      "Gradient norm for subject_5: 0.0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: False\n",
      "Biases changed for subject_1: False\n",
      "\n",
      "Weights changed for subject_2: False\n",
      "Biases changed for subject_2: False\n",
      "\n",
      "Weights changed for subject_3: False\n",
      "Biases changed for subject_3: False\n",
      "\n",
      "Weights changed for subject_4: False\n",
      "Biases changed for subject_4: False\n",
      "\n",
      "Weights changed for subject_5: False\n",
      "Biases changed for subject_5: False\n",
      "\n",
      "Weights changed for subject_6: False\n",
      "Biases changed for subject_6: False\n",
      "\n",
      "Weights changed for subject_7: False\n",
      "Biases changed for subject_7: False\n",
      "\n",
      "Weights changed for subject_8: False\n",
      "Biases changed for subject_8: False\n",
      "\n",
      "Weights changed for subject_9: False\n",
      "Biases changed for subject_9: False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Weights for subject_3 did not change!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m biases_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(st_layer\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata, initial_biases[subject_id], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subject_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecific_subject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m weights_changed, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m did not change!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m biases_changed, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBiases for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m did not change!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Weights for subject_3 did not change!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming SubjectSpecificConvNet is defined as before\n",
    "class SubjectSpecificConvNet(nn.Module):\n",
    "    def __init__(self, n_chans, n_outputs, n_times=1001, dropout=0.5, num_kernels=40, \n",
    "                 kernel_size=25, pool_size=100, num_subjects=9):\n",
    "        super(SubjectSpecificConvNet, self).__init__()\n",
    "        self.num_subjects = num_subjects\n",
    "\n",
    "        # Create a dictionary of spatio-temporal convolutional layers, one per subject\n",
    "        self.spatio_temporal_layers = nn.ModuleDict({\n",
    "            f'subject_{i+1}': nn.Conv2d(n_chans, num_kernels, (1, kernel_size))\n",
    "            for i in range(num_subjects)\n",
    "        })\n",
    "\n",
    "        self.pool = nn.AvgPool2d((1, pool_size))\n",
    "        self.batch_norm = nn.BatchNorm2d(num_kernels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.LazyLinear(n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract subject IDs from the last time point of the first channel\n",
    "        subject_ids = x[:, 0, -1] / 1000000  # Assuming subject IDs are stored scaled in the last time point\n",
    "        x = x[:, :, :-1]  # Remove the last time point (which contains the subject ID)\n",
    "\n",
    "        # Add dimension for Conv2d\n",
    "        x = torch.unsqueeze(x, dim=2)  # Shape: (batch_size, n_chans, 1, n_times)\n",
    "\n",
    "        # Prepare a list to collect outputs\n",
    "        conv_outputs = []\n",
    "\n",
    "        for i in range(x.size(0)):  # Loop over batch size\n",
    "            subject_id = int(subject_ids[i].item())\n",
    "            # Select the appropriate convolutional layer\n",
    "            conv_layer = self.spatio_temporal_layers[f'subject_{subject_id}']\n",
    "            # Apply the convolutional layer to the i-th sample\n",
    "            xi = x[i].unsqueeze(0)  # Add batch dimension\n",
    "            xi = conv_layer(xi)\n",
    "            conv_outputs.append(xi)\n",
    "\n",
    "        # Stack the outputs along the batch dimension\n",
    "        x = torch.cat(conv_outputs, dim=0)\n",
    "\n",
    "        x = F.elu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SubjectSpecificConvNet(n_chans=22, n_outputs=4)\n",
    "\n",
    "# Fill spatio-temporal layer weights and biases with 1.0\n",
    "for st_layer in model.spatio_temporal_layers.values():\n",
    "    st_layer.weight.data.fill_(1.0)\n",
    "    st_layer.bias.data.fill_(1.0)\n",
    "\n",
    "# Set requires_grad=False for all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Set requires_grad=True for parameters in spatio_temporal_layers\n",
    "for st_layer in model.spatio_temporal_layers.values():\n",
    "    for param in st_layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "print(\"Parameter requires_grad status:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Copy initial weights and biases\n",
    "initial_weights = {}\n",
    "initial_biases = {}\n",
    "for subject_id, st_layer in model.spatio_temporal_layers.items():\n",
    "    initial_weights[subject_id] = st_layer.weight.data.clone()\n",
    "    initial_biases[subject_id] = st_layer.bias.data.clone()\n",
    "\n",
    "# Prepare input data\n",
    "batch_size = 5\n",
    "n_chans = 22\n",
    "n_times = 1001\n",
    "\n",
    "# Create a batch with zeros\n",
    "batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "\n",
    "# Define target outputs\n",
    "n_outputs = model.fc.out_features\n",
    "target = torch.zeros(batch_size, n_outputs)\n",
    "\n",
    "# Use a non-zero input for one specific subject\n",
    "specific_subject_id = 3  # Subject ID for which we want to induce updates\n",
    "batch[specific_subject_id - 1] = 1  # Fill with ones for the specific subject\n",
    "\n",
    "# Set the last time point to contain the subject IDs (from 1 to batch_size)\n",
    "for i in range(batch_size):\n",
    "    subject_id = i + 1  # Subject IDs from 1 to 5\n",
    "    batch[i, :, -1] = subject_id * 1000000  # Multiply by 1,000,000 to match extraction in forward()\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define an optimizer (only parameters with requires_grad=True will be updated)\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "\n",
    "# Training step\n",
    "model.train()\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "output = model(batch)\n",
    "\n",
    "# Compute loss\n",
    "loss = criterion(output, target)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Verify gradients before optimizer step\n",
    "print(\"\\nGradients before optimizer step:\")\n",
    "for subject_id, st_layer in model.spatio_temporal_layers.items():\n",
    "    grad_norm = st_layer.weight.grad.norm().item() if st_layer.weight.grad is not None else 0\n",
    "    print(f\"Gradient norm for {subject_id}: {grad_norm}\")\n",
    "\n",
    "# Update weights\n",
    "optimizer.step()\n",
    "\n",
    "# Check which spatio_temporal_layers have had their weights updated\n",
    "for subject_id, st_layer in model.spatio_temporal_layers.items():\n",
    "    weights_changed = not torch.allclose(st_layer.weight.data, initial_weights[subject_id], atol=1e-6)\n",
    "    biases_changed = not torch.allclose(st_layer.bias.data, initial_biases[subject_id], atol=1e-6)\n",
    "    print(f\"\\nWeights changed for {subject_id}: {weights_changed}\")\n",
    "    print(f\"Biases changed for {subject_id}: {biases_changed}\")\n",
    "\n",
    "# Assertions to verify subject specificity\n",
    "for subject_id, st_layer in model.spatio_temporal_layers.items():\n",
    "    weights_changed = not torch.allclose(st_layer.weight.data, initial_weights[subject_id], atol=1e-6)\n",
    "    biases_changed = not torch.allclose(st_layer.bias.data, initial_biases[subject_id], atol=1e-6)\n",
    "    if subject_id == f'subject_{specific_subject_id}':\n",
    "        assert weights_changed, f\"Weights for {subject_id} did not change!\"\n",
    "        assert biases_changed, f\"Biases for {subject_id} did not change!\"\n",
    "    else:\n",
    "        assert not weights_changed, f\"Weights for {subject_id} have changed!\"\n",
    "        assert not biases_changed, f\"Biases for {subject_id} have changed!\"\n",
    "\n",
    "print(\"\\nTest passed: Only the weights and biases corresponding to the specific subject have changed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
