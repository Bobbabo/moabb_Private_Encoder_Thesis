{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.5\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0\n",
      "Gradient norm for subject_3: 0.0\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 0.0\n",
      "Biases changed for subject_2: 0.0\n",
      "\n",
      "Weights changed for subject_3: 0.0\n",
      "Biases changed for subject_3: 3.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Weights for subject_3 did not change!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m batch_subject_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     99\u001b[0m target_subject_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m--> 100\u001b[0m \u001b[43mtest_subject_specificity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_subject_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_subject_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Test with two specific subjects\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# batch_subject_ids = [1, 2, 3, 4, 5]\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# target_subject_ids = [2,3]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# target_subject_ids = [1,2,3,4]\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# test_subject_specificity(batch_subject_ids, target_subject_ids)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 88\u001b[0m, in \u001b[0;36mtest_subject_specificity\u001b[0;34m(batch_subject_ids, target_subject_ids, n_chans, n_times, n_outputs)\u001b[0m\n\u001b[1;32m     86\u001b[0m biases_changed \u001b[38;5;241m=\u001b[39m (fc_layer\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m!=\u001b[39m initial_biases[subject_id_str])\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subject_id_int \u001b[38;5;129;01min\u001b[39;00m target_subject_ids:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m weights_changed \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m did not change!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m biases_changed \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBiases for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m did not change!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Weights for subject_3 did not change!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shallowDict import SubjectDicionaryFCNet\n",
    "\n",
    "def test_subject_specificity(batch_subject_ids, target_subject_ids, n_chans=22, n_times=1001, n_outputs=4):\n",
    "    # Initialize the model\n",
    "    model = SubjectDicionaryFCNet(n_chans=n_chans, n_outputs=n_outputs)\n",
    "    \n",
    "    # Set all weights and biases of fc_layers to ones for consistency\n",
    "    for fc_layer in model.fc_layers.values():\n",
    "        fc_layer.weight.data.fill_(1.0)\n",
    "        fc_layer.bias.data.fill_(1.0)\n",
    "    \n",
    "    # Set requires_grad appropriately\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for fc_layer in model.fc_layers.values():\n",
    "        for param in fc_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Copy initial weights and biases\n",
    "    initial_weights = {k: v.weight.data.clone() for k, v in model.fc_layers.items()}\n",
    "    initial_biases = {k: v.bias.data.clone() for k, v in model.fc_layers.items()}\n",
    "    \n",
    "    batch_size = len(batch_subject_ids)\n",
    "    \n",
    "    # Create batch data\n",
    "    batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "    \n",
    "    # Initialize target\n",
    "    target = torch.zeros(batch_size, n_outputs)\n",
    "    \n",
    "    # Set batch data and target\n",
    "    for i, subject_id in enumerate(batch_subject_ids):\n",
    "        if subject_id in target_subject_ids:\n",
    "            batch[i] = 1.0  # Or any meaningful data\n",
    "            target[i] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Non-zero target\n",
    "        else:\n",
    "            batch[i] = 0.0  # Or any other data\n",
    "            target[i] = 0.0  # Zero target\n",
    "        \n",
    "        # Correctly encode the subject ID\n",
    "        batch[i, :, -1] = subject_id * 1000000\n",
    "\n",
    "    # Define a loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Define an optimizer\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(batch)\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "    # Print loss value\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Verify gradients before optimizer step\n",
    "    print(\"\\nGradients before optimizer step:\")\n",
    "    for subject_id_str, fc_layer in model.fc_layers.items():\n",
    "        grad_norm = fc_layer.weight.grad.norm().item() if fc_layer.weight.grad is not None else 0\n",
    "        print(f\"Gradient norm for {subject_id_str}: {grad_norm}\")\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check which fc_layers have had their weights updated\n",
    "    for subject_id_str, fc_layer in model.fc_layers.items():\n",
    "        weights_changed = (fc_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (fc_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        print(f\"\\nWeights changed for {subject_id_str}: {weights_changed}\")\n",
    "        print(f\"Biases changed for {subject_id_str}: {biases_changed}\")\n",
    "    \n",
    "    # Assertions to verify subject specificity\n",
    "    for subject_id_str, fc_layer in model.fc_layers.items():\n",
    "        subject_id_int = int(subject_id_str.split('_')[1])  # Convert 'subject_1' to 1\n",
    "        weights_changed = (fc_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (fc_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        if subject_id_int in target_subject_ids:\n",
    "            assert weights_changed > 0, f\"Weights for {subject_id_str} did not change!\"\n",
    "            assert biases_changed > 0, f\"Biases for {subject_id_str} did not change!\"\n",
    "        else:\n",
    "            assert weights_changed == 0, f\"Weights for {subject_id_str} have changed!\"\n",
    "            assert biases_changed == 0, f\"Biases for {subject_id_str} have changed!\"\n",
    "    \n",
    "    print(\"\\nTest passed: Only the weights and biases corresponding to the specified subjects have changed.\")\n",
    "\n",
    "\n",
    "# Test with one specific subject\n",
    "batch_subject_ids = [3]\n",
    "target_subject_ids = [3]\n",
    "test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# Test with two specific subjects\n",
    "# batch_subject_ids = [1, 2, 3, 4, 5]\n",
    "# target_subject_ids = [2,3]\n",
    "# test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# # Test with all but 1 ssubject\n",
    "# batch_subject_ids = [1, 2, 3, 4, 5]\n",
    "# target_subject_ids = [1,2,3,4]\n",
    "# test_subject_specificity(batch_subject_ids, target_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.404048919677734\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0\n",
      "Gradient norm for subject_3: 0.024984247982501984\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 0.0\n",
      "Biases changed for subject_2: 0.0\n",
      "\n",
      "Weights changed for subject_3: 20906.0\n",
      "Biases changed for subject_3: 35.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n",
      "\n",
      "Test passed: Only the weights and biases corresponding to the specified subjects have changed.\n",
      "Loss: 7.613008499145508\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0.03848487511277199\n",
      "Gradient norm for subject_3: 0\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 21444.0\n",
      "Biases changed for subject_2: 38.0\n",
      "\n",
      "Weights changed for subject_3: 0.0\n",
      "Biases changed for subject_3: 0.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n",
      "\n",
      "Test passed: Only the weights and biases corresponding to the specified subjects have changed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shallowDict import ShallowPrivateCollapsedDictNetSlow\n",
    "\n",
    "def test_subject_specificity(batch_subject_ids, target_subject_ids, n_chans=22, n_times=1001, n_outputs=4):\n",
    "    # Initialize the model\n",
    "    model = ShallowPrivateCollapsedDictNetSlow(n_chans=n_chans, n_outputs=n_outputs)\n",
    "    \n",
    "    # Set all weights and biases of fc_layers to ones for consistency\n",
    "    for spatio_temporal_layer in model.spatio_temporal_layers.values():\n",
    "        spatio_temporal_layer.weight.data.fill_(1.0)\n",
    "        spatio_temporal_layer.bias.data.fill_(1.0)\n",
    "    \n",
    "    # Set requires_grad appropriately\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for spatio_temporal_layer in model.spatio_temporal_layers.values():\n",
    "        for param in spatio_temporal_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    #for param in model.batch_norm.parameters():\n",
    "    #    param.requires_grad = True\n",
    "    \n",
    "    # Copy initial weights and biases\n",
    "    initial_weights = {k: v.weight.data.clone() for k, v in model.spatio_temporal_layers.items()}\n",
    "    initial_biases = {k: v.bias.data.clone() for k, v in model.spatio_temporal_layers.items()}\n",
    "    \n",
    "    batch_size = len(batch_subject_ids)\n",
    "    \n",
    "    # Create batch data\n",
    "    batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "    \n",
    "    # Initialize target\n",
    "    target = torch.zeros(batch_size, n_outputs)\n",
    "    \n",
    "    # Set batch data and target\n",
    "    for i, subject_id in enumerate(batch_subject_ids):\n",
    "        if subject_id in target_subject_ids:\n",
    "            batch[i] = 1.0  # Or any meaningful data\n",
    "            target[i] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Non-zero target\n",
    "        else:\n",
    "            batch[i] = 0.0  # Or any other data\n",
    "            target[i] = 0.0  # Zero target\n",
    "        \n",
    "        # Correctly encode the subject ID\n",
    "        batch[i, :, -1] = subject_id * 1000000\n",
    "\n",
    "    # Define a loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Define an optimizer\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(batch)\n",
    "    \n",
    "\n",
    "    # Compute loss only for target subjects\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    \n",
    "    # Print loss value\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Verify gradients before optimizer step\n",
    "    print(\"\\nGradients before optimizer step:\")\n",
    "    for subject_id_str, spatio_temporal_layer in model.spatio_temporal_layers.items():\n",
    "        grad_norm = spatio_temporal_layer.weight.grad.norm().item() if spatio_temporal_layer.weight.grad is not None else 0\n",
    "        print(f\"Gradient norm for {subject_id_str}: {grad_norm}\")\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check which fc_layers have had their weights updated\n",
    "    for subject_id_str, spatio_temporal_layer in model.spatio_temporal_layers.items():\n",
    "        weights_changed = (spatio_temporal_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (spatio_temporal_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        print(f\"\\nWeights changed for {subject_id_str}: {weights_changed}\")\n",
    "        print(f\"Biases changed for {subject_id_str}: {biases_changed}\")\n",
    "    \n",
    "    # Assertions to verify subject specificity\n",
    "    for subject_id_str, spatio_temporal_layer in model.spatio_temporal_layers.items():\n",
    "        subject_id_int = int(subject_id_str.split('_')[1])  # Convert 'subject_1' to 1\n",
    "        weights_changed = (spatio_temporal_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (spatio_temporal_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        if subject_id_int in target_subject_ids:\n",
    "            assert weights_changed > 0, f\"Weights for {subject_id_str} did not change!\"\n",
    "            assert biases_changed > 0, f\"Biases for {subject_id_str} did not change!\"\n",
    "        else:\n",
    "            assert weights_changed == 0, f\"Weights for {subject_id_str} have changed!\"\n",
    "            assert biases_changed == 0, f\"Biases for {subject_id_str} have changed!\"\n",
    "    \n",
    "    print(\"\\nTest passed: Only the weights and biases corresponding to the specified subjects have changed.\")\n",
    "\n",
    "\n",
    "# Test with one specific subject\n",
    "batch_subject_ids = [3]\n",
    "target_subject_ids = [3]\n",
    "test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# Test with two specific subjects\n",
    "batch_subject_ids = [2]\n",
    "target_subject_ids = [2]\n",
    "test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# # Test with all but 1 ssubject\n",
    "# batch_subject_ids = [1, 2, 3, 4, 5]\n",
    "# target_subject_ids = [1,2,3,4]\n",
    "# test_subject_specificity(batch_subject_ids, target_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.641894340515137\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0\n",
      "Gradient norm for subject_3: 0.002165168523788452\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 0.0\n",
      "Biases changed for subject_2: 0.0\n",
      "\n",
      "Weights changed for subject_3: 1000.0\n",
      "Biases changed for subject_3: 37.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n",
      "\n",
      "Test passed: Only the weights and biases corresponding to the specified subjects have changed.\n",
      "Loss: 7.4312663078308105\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0.002657759701833129\n",
      "Gradient norm for subject_3: 0\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 975.0\n",
      "Biases changed for subject_2: 32.0\n",
      "\n",
      "Weights changed for subject_3: 0.0\n",
      "Biases changed for subject_3: 0.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n",
      "\n",
      "Test passed: Only the weights and biases corresponding to the specified subjects have changed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shallowDict import ShallowPrivateTemporalDictNetSlow\n",
    "\n",
    "def test_subject_specificity(batch_subject_ids, target_subject_ids, n_chans=22, n_times=1001, n_outputs=4):\n",
    "    # Initialize the model\n",
    "    model = ShallowPrivateTemporalDictNetSlow(n_chans=n_chans, n_outputs=n_outputs)\n",
    "    \n",
    "    # Set all weights and biases of fc_layers to ones for consistency\n",
    "    for temporal_layer in model.temporal_layers.values():\n",
    "        temporal_layer.weight.data.fill_(1.0)\n",
    "        temporal_layer.bias.data.fill_(1.0)\n",
    "    \n",
    "    # Set requires_grad appropriately\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for temporal_layer in model.temporal_layers.values():\n",
    "        for param in temporal_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    #for param in model.batch_norm.parameters():\n",
    "    #    param.requires_grad = True\n",
    "    \n",
    "    # Copy initial weights and biases\n",
    "    initial_weights = {k: v.weight.data.clone() for k, v in model.temporal_layers.items()}\n",
    "    initial_biases = {k: v.bias.data.clone() for k, v in model.temporal_layers.items()}\n",
    "    \n",
    "    batch_size = len(batch_subject_ids)\n",
    "    \n",
    "    # Create batch data\n",
    "    batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "    \n",
    "    # Initialize target\n",
    "    target = torch.zeros(batch_size, n_outputs)\n",
    "    \n",
    "    # Set batch data and target\n",
    "    for i, subject_id in enumerate(batch_subject_ids):\n",
    "        if subject_id in target_subject_ids:\n",
    "            batch[i] = 1.0  # Or any meaningful data\n",
    "            target[i] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Non-zero target\n",
    "        else:\n",
    "            batch[i] = 0.0  # Or any other data\n",
    "            target[i] = 0.0  # Zero target\n",
    "        \n",
    "        # Correctly encode the subject ID\n",
    "        batch[i, :, -1] = subject_id * 1000000\n",
    "\n",
    "    # Define a loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Define an optimizer\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(batch)\n",
    "    \n",
    "\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Print loss value\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Verify gradients before optimizer step\n",
    "    print(\"\\nGradients before optimizer step:\")\n",
    "    for subject_id_str, temporal_layer in model.temporal_layers.items():\n",
    "        grad_norm = temporal_layer.weight.grad.norm().item() if temporal_layer.weight.grad is not None else 0\n",
    "        print(f\"Gradient norm for {subject_id_str}: {grad_norm}\")\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check which fc_layers have had their weights updated\n",
    "    for subject_id_str, temporal_layer in model.temporal_layers.items():\n",
    "        weights_changed = (temporal_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (temporal_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        print(f\"\\nWeights changed for {subject_id_str}: {weights_changed}\")\n",
    "        print(f\"Biases changed for {subject_id_str}: {biases_changed}\")\n",
    "    \n",
    "    # Assertions to verify subject specificity\n",
    "    for subject_id_str, temporal_layer in model.temporal_layers.items():\n",
    "        subject_id_int = int(subject_id_str.split('_')[1])  # Convert 'subject_1' to 1\n",
    "        weights_changed = (temporal_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (temporal_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        if subject_id_int in target_subject_ids:\n",
    "            assert weights_changed > 0, f\"Weights for {subject_id_str} did not change!\"\n",
    "            assert biases_changed > 0, f\"Biases for {subject_id_str} did not change!\"\n",
    "        else:\n",
    "            assert weights_changed == 0, f\"Weights for {subject_id_str} have changed!\"\n",
    "            assert biases_changed == 0, f\"Biases for {subject_id_str} have changed!\"\n",
    "    \n",
    "    print(\"\\nTest passed: Only the weights and biases corresponding to the specified subjects have changed.\")\n",
    "\n",
    "\n",
    "# Test with one specific subject\n",
    "batch_subject_ids = [3]\n",
    "target_subject_ids = [3]\n",
    "test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# Test with two specific subjects\n",
    "batch_subject_ids = [2]\n",
    "target_subject_ids = [2]\n",
    "test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# # Test with all but 1 ssubject\n",
    "# batch_subject_ids = [1, 2, 3, 4, 5]\n",
    "# target_subject_ids = [1,2,3,4]\n",
    "# test_subject_specificity(batch_subject_ids, target_subject_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.432461261749268\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0\n",
      "Gradient norm for subject_3: 0.0324491485953331\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 0.0\n",
      "Biases changed for subject_2: 0.0\n",
      "\n",
      "Weights changed for subject_3: 31790.0\n",
      "Biases changed for subject_3: 39.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n",
      "\n",
      "Test passed: Only the weights and biases corresponding to the specified subjects have changed.\n",
      "Loss: 7.571025848388672\n",
      "\n",
      "Gradients before optimizer step:\n",
      "Gradient norm for subject_1: 0\n",
      "Gradient norm for subject_2: 0.054030124098062515\n",
      "Gradient norm for subject_3: 0\n",
      "Gradient norm for subject_4: 0\n",
      "Gradient norm for subject_5: 0\n",
      "Gradient norm for subject_6: 0\n",
      "Gradient norm for subject_7: 0\n",
      "Gradient norm for subject_8: 0\n",
      "Gradient norm for subject_9: 0\n",
      "\n",
      "Weights changed for subject_1: 0.0\n",
      "Biases changed for subject_1: 0.0\n",
      "\n",
      "Weights changed for subject_2: 30888.0\n",
      "Biases changed for subject_2: 39.0\n",
      "\n",
      "Weights changed for subject_3: 0.0\n",
      "Biases changed for subject_3: 0.0\n",
      "\n",
      "Weights changed for subject_4: 0.0\n",
      "Biases changed for subject_4: 0.0\n",
      "\n",
      "Weights changed for subject_5: 0.0\n",
      "Biases changed for subject_5: 0.0\n",
      "\n",
      "Weights changed for subject_6: 0.0\n",
      "Biases changed for subject_6: 0.0\n",
      "\n",
      "Weights changed for subject_7: 0.0\n",
      "Biases changed for subject_7: 0.0\n",
      "\n",
      "Weights changed for subject_8: 0.0\n",
      "Biases changed for subject_8: 0.0\n",
      "\n",
      "Weights changed for subject_9: 0.0\n",
      "Biases changed for subject_9: 0.0\n",
      "\n",
      "Test passed: Only the weights and biases corresponding to the specified subjects have changed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from shallowDict import ShallowPrivateSpatialDictNetSlow\n",
    "\n",
    "def test_subject_specificity(batch_subject_ids, target_subject_ids, n_chans=22, n_times=1001, n_outputs=4):\n",
    "    # Initialize the model\n",
    "    model = ShallowPrivateSpatialDictNetSlow(n_chans=n_chans, n_outputs=n_outputs)\n",
    "    \n",
    "    # Set all weights and biases of fc_layers to ones for consistency\n",
    "    for spatial_layer in model.spatial_layers.values():\n",
    "        spatial_layer.weight.data.fill_(1.0)\n",
    "        spatial_layer.bias.data.fill_(1.0)\n",
    "    \n",
    "    # Set requires_grad appropriately\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for spatial_layer in model.spatial_layers.values():\n",
    "        for param in spatial_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "    #for param in model.batch_norm.parameters():\n",
    "    #    param.requires_grad = True\n",
    "    \n",
    "    # Copy initial weights and biases\n",
    "    initial_weights = {k: v.weight.data.clone() for k, v in model.spatial_layers.items()}\n",
    "    initial_biases = {k: v.bias.data.clone() for k, v in model.spatial_layers.items()}\n",
    "    \n",
    "    batch_size = len(batch_subject_ids)\n",
    "    \n",
    "    # Create batch data\n",
    "    batch = torch.zeros(batch_size, n_chans, n_times)\n",
    "    \n",
    "    # Initialize target\n",
    "    target = torch.zeros(batch_size, n_outputs)\n",
    "    \n",
    "    # Set batch data and target\n",
    "    for i, subject_id in enumerate(batch_subject_ids):\n",
    "        if subject_id in target_subject_ids:\n",
    "            batch[i] = 1.0  # Or any meaningful data\n",
    "            target[i] = torch.tensor([1.0, 2.0, 3.0, 4.0])  # Non-zero target\n",
    "        else:\n",
    "            batch[i] = 0.0  # Or any other data\n",
    "            target[i] = 0.0  # Zero target\n",
    "        \n",
    "        # Correctly encode the subject ID\n",
    "        batch[i, :, -1] = subject_id * 1000000\n",
    "\n",
    "    # Define a loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Define an optimizer\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01)\n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(batch)\n",
    "    \n",
    "    # Compute loss only for target subjects\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    \n",
    "    # Print loss value\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Verify gradients before optimizer step\n",
    "    print(\"\\nGradients before optimizer step:\")\n",
    "    for subject_id_str, spatial_layer in model.spatial_layers.items():\n",
    "        grad_norm = spatial_layer.weight.grad.norm().item() if spatial_layer.weight.grad is not None else 0\n",
    "        print(f\"Gradient norm for {subject_id_str}: {grad_norm}\")\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Check which fc_layers have had their weights updated\n",
    "    for subject_id_str, spatial_layer in model.spatial_layers.items():\n",
    "        weights_changed = (spatial_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (spatial_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        print(f\"\\nWeights changed for {subject_id_str}: {weights_changed}\")\n",
    "        print(f\"Biases changed for {subject_id_str}: {biases_changed}\")\n",
    "    \n",
    "    # Assertions to verify subject specificity\n",
    "    for subject_id_str, spatial_layer in model.spatial_layers.items():\n",
    "        subject_id_int = int(subject_id_str.split('_')[1])  # Convert 'subject_1' to 1\n",
    "        weights_changed = (spatial_layer.weight.data != initial_weights[subject_id_str]).float().sum().item()\n",
    "        biases_changed = (spatial_layer.bias.data != initial_biases[subject_id_str]).float().sum().item()\n",
    "        if subject_id_int in target_subject_ids:\n",
    "            assert weights_changed > 0, f\"Weights for {subject_id_str} did not change!\"\n",
    "            assert biases_changed > 0, f\"Biases for {subject_id_str} did not change!\"\n",
    "        else:\n",
    "            assert weights_changed == 0, f\"Weights for {subject_id_str} have changed!\"\n",
    "            assert biases_changed == 0, f\"Biases for {subject_id_str} have changed!\"\n",
    "    \n",
    "    print(\"\\nTest passed: Only the weights and biases corresponding to the specified subjects have changed.\")\n",
    "\n",
    "\n",
    "# Test with one specific subject\n",
    "batch_subject_ids = [3]\n",
    "target_subject_ids = [3]\n",
    "test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# Test with two specific subjects\n",
    "batch_subject_ids = [2]\n",
    "target_subject_ids = [2]\n",
    "test_subject_specificity(batch_subject_ids, target_subject_ids)\n",
    "\n",
    "# Test with all but 1 ssubject\n",
    "# batch_subject_ids = [1, 2, 3, 4, 5]\n",
    "# target_subject_ids = [1,2,3,4]\n",
    "# test_subject_specificity(batch_subject_ids, target_subject_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
